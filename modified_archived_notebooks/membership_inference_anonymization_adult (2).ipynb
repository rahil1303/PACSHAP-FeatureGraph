{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8afe274a",
   "metadata": {},
   "source": [
    "# Using ML Anonymization to Defend Against Membership Inference Attacks\n",
    "In this tutorial, we will show how to anonymize models using the ML anonymization module.\n",
    "\n",
    "We will demonstrate running inference attacks both on a vanilla model and on an anonymized version of the model. We will run a black-box membership inference attack using ART's inference module.\n",
    "\n",
    "Dataset used: [Adult dataset](https://archive.ics.uci.edu/ml/datasets/adult).\n",
    "\n",
    "For simplicity, only numerical features are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dab63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ../datasets\n",
    "!wget -P ../datasets https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\n",
    "!wget -P ../datasets https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data\n",
    "!wget -P ../datasets https://archive.ics.uci.edu/ml/machine-learning-databases/nursery/nursery.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d95c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('../datasets/adult.data', header=None, skipinitialspace=True)\n",
    "num_features = [0, 4, 10, 11, 12]\n",
    "X = data.iloc[:, num_features].values\n",
    "y = data.iloc[:, 14].values\n",
    "\n",
    "# Cleaning labels\n",
    "y = np.array([label.strip() for label in y])\n",
    "y = np.where(y == '<=50K', 0, 1)\n",
    "\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "y_train = y_train.astype(int)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "print(f'Final Train Set: {x_train.shape}, Test Set: {x_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746bb500",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install adversarial-robustness-toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba84114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from art.estimators.classification.scikitlearn import ScikitlearnDecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "art_classifier = ScikitlearnDecisionTreeClassifier(model)\n",
    "\n",
    "print('Base model accuracy: ', model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc72bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.inference.membership_inference import MembershipInferenceBlackBox\n",
    "\n",
    "bb_attack = MembershipInferenceBlackBox(art_classifier, attack_model_type='rf')\n",
    "\n",
    "attack_train_ratio = 0.5\n",
    "attack_train_size = int(len(x_train) * attack_train_ratio)\n",
    "attack_test_size = int(len(x_test) * attack_train_ratio)\n",
    "\n",
    "bb_attack.fit(x_train[:attack_train_size], y_train[:attack_train_size],\n",
    "              x_test[:attack_test_size], y_test[:attack_test_size])\n",
    "\n",
    "inferred_train_bb = bb_attack.infer(x_train[attack_train_size:], y_train[attack_train_size:])\n",
    "inferred_test_bb = bb_attack.infer(x_test[attack_test_size:], y_test[attack_test_size:])\n",
    "\n",
    "train_acc = np.sum(inferred_train_bb) / len(inferred_train_bb)\n",
    "test_acc = 1 - (np.sum(inferred_test_bb) / len(inferred_test_bb))\n",
    "\n",
    "acc = (train_acc * len(inferred_train_bb) + test_acc * len(inferred_test_bb)) / (len(inferred_train_bb) + len(inferred_test_bb))\n",
    "\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0df631",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ai-privacy-toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4352cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from apt.utils.datasets import ArrayDataset\n",
    "from apt.anonymization import Anonymize\n",
    "\n",
    "QI = [0, 1, 2, 4]\n",
    "anonymizer = Anonymize(100, QI)\n",
    "anon = anonymizer.anonymize(ArrayDataset(x_train, np.array([np.argmax(arr) for arr in art_classifier.predict(x_train)])))\n",
    "\n",
    "print(anon)\n",
    "\n",
    "anon_model = DecisionTreeClassifier()\n",
    "anon_model.fit(anon, y_train)\n",
    "anon_art_classifier = ScikitlearnDecisionTreeClassifier(anon_model)\n",
    "\n",
    "print('Anonymized model accuracy: ', anon_model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeba009",
   "metadata": {},
   "outputs": [],
   "source": [
    "anon_bb_attack = MembershipInferenceBlackBox(anon_art_classifier, attack_model_type='rf')\n",
    "\n",
    "anon_bb_attack.fit(x_train[:attack_train_size], y_train[:attack_train_size],\n",
    "                   x_test[:attack_test_size], y_test[:attack_test_size])\n",
    "\n",
    "anon_inferred_train_bb = anon_bb_attack.infer(x_train[attack_train_size:], y_train[attack_train_size:])\n",
    "anon_inferred_test_bb = anon_bb_attack.infer(x_test[attack_test_size:], y_test[attack_test_size:])\n",
    "\n",
    "anon_train_acc = np.sum(anon_inferred_train_bb) / len(anon_inferred_train_bb)\n",
    "anon_test_acc = 1 - (np.sum(anon_inferred_test_bb) / len(anon_inferred_test_bb))\n",
    "\n",
    "anon_acc = (anon_train_acc * len(anon_inferred_train_bb) + anon_test_acc * len(anon_inferred_test_bb)) / (len(anon_inferred_train_bb) + len(anon_inferred_test_bb))\n",
    "\n",
    "print(anon_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276ee083",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_precision_recall(predicted, actual, positive_value=1):\n",
    "    score = 0\n",
    "    num_positive_predicted = sum(predicted == positive_value)\n",
    "    num_positive_actual = sum(actual == positive_value)\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted[i] == actual[i] and predicted[i] == positive_value:\n",
    "            score += 1\n",
    "    precision = score / num_positive_predicted if num_positive_predicted else 1\n",
    "    recall = score / num_positive_actual if num_positive_actual else 1\n",
    "    return precision, recall\n",
    "\n",
    "print('without anonymization:', calc_precision_recall(np.concatenate((inferred_train_bb, inferred_test_bb)),\n",
    "                        np.concatenate((np.ones(len(inferred_train_bb)), np.zeros(len(inferred_test_bb))))))\n",
    "\n",
    "print('with anonymization:', calc_precision_recall(np.concatenate((anon_inferred_train_bb, anon_inferred_test_bb)),\n",
    "                        np.concatenate((np.ones(len(anon_inferred_train_bb)), np.zeros(len(anon_inferred_test_bb))))))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
